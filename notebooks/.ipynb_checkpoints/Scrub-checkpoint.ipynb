{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary ibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T17:26:43.292312Z",
     "start_time": "2019-06-27T17:26:37.042017Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import pickle_to, pickle_from, ignore_warnings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub data\n",
    "###### Encoding newly created variable names into 0 or 1\n",
    "\n",
    "Encoding | Label\n",
    "-- | --\n",
    "0 | FAKE\n",
    "1 | REAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T17:26:45.801841Z",
     "start_time": "2019-06-27T17:26:45.790880Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    \"\"\"Encode real as 1, fake as 0, everything else as None\"\"\"\n",
    "    if label == \"REAL\":\n",
    "        return 1\n",
    "    elif label == \"FAKE\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T17:26:47.003654Z",
     "start_time": "2019-06-27T17:26:46.904139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file from ../data/raw/raw.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "raw = pickle_from('../data/raw/raw.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:50:41.519386Z",
     "start_time": "2019-06-25T21:50:41.482341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply label encoding function to raw data\n",
    "\n",
    "raw['numeric_label'] = raw['label'].apply(encode_label)\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:50:53.135169Z",
     "start_time": "2019-06-25T21:50:53.044053Z"
    }
   },
   "outputs": [],
   "source": [
    "raw[raw.text.str.contains('turtle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:01.582609Z",
     "start_time": "2019-06-25T21:51:01.504639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing rows that have the string 'sweeping consequences'\n",
    "# These have been observed to be duplicates of a single line\n",
    "\n",
    "raw = raw[~raw['text'].str.contains('sweeping consequence')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:02.261516Z",
     "start_time": "2019-06-25T21:51:02.187982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing rows that have the string 'automatically' \n",
    "# as this is part of html code\n",
    "\n",
    "raw = raw[~raw['text'].str.contains('automatically')]\n",
    "raw = raw.drop(220).reset_index()\n",
    "raw = raw.drop('index',axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:02.978442Z",
     "start_time": "2019-06-25T21:51:02.963351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "data = raw[['title','text','numeric_label']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:23:47.866417Z",
     "start_time": "2019-06-14T04:23:47.847592Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining text and title data into a single column by name 'news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:31.884044Z",
     "start_time": "2019-06-25T21:51:31.804054Z"
    }
   },
   "outputs": [],
   "source": [
    "data['news'] = data['title'] + '. '  + data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:32.938384Z",
     "start_time": "2019-06-25T21:51:32.917586Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "Involves the following <br>\n",
    "\n",
    "* converting everyting to lowercase\n",
    "* removing punctuations\n",
    "* removing numbers\n",
    "* removing non english words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:37.870912Z",
     "start_time": "2019-06-25T21:51:36.388326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting to lowercase\n",
    "data.text = data.text.apply(lambda x:x.lower())\n",
    "data['text'] = data['text'].str.replace(\"â€™\",\"'\")\n",
    "\n",
    "\n",
    "#Removing all punctuations\n",
    "data.text = data.text.str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Removing numbers\n",
    "data.text = data.text.str.replace('\\d+', ' ')\n",
    "\n",
    "# Making sure any double-spaces are single\n",
    "data.text = data.text.str.replace('  ',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T21:51:43.857709Z",
     "start_time": "2019-06-25T21:51:43.826867Z"
    }
   },
   "outputs": [],
   "source": [
    "data.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:23:55.836478Z",
     "start_time": "2019-06-14T04:23:55.824787Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = data[['numeric_label','text']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:23:56.934278Z",
     "start_time": "2019-06-14T04:23:56.922917Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T02:37:04.062200Z",
     "start_time": "2019-06-14T02:37:04.048067Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:12.195334Z",
     "start_time": "2019-06-14T04:24:12.187827Z"
    }
   },
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:33.058665Z",
     "start_time": "2019-06-14T04:24:12.820073Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data['tokenized'] = text_data.text.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:33.088273Z",
     "start_time": "2019-06-14T04:24:33.061402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_data.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:33.114350Z",
     "start_time": "2019-06-14T04:24:33.094689Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:33.125037Z",
     "start_time": "2019-06-14T04:24:33.117428Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words.append('tweet')\n",
    "stop_words.append('home')\n",
    "stop_words.append('headlines')\n",
    "stop_words.append('finance')\n",
    "stop_words.append('news')\n",
    "stop_words.append('was')\n",
    "stop_words.append('has')\n",
    "stop_words.append('said')\n",
    "stop_words.append('wa')\n",
    "stop_words.append('ha')\n",
    "stop_words.append('leave')\n",
    "stop_words.append('comment')\n",
    "stop_words.append('loading')\n",
    "stop_words.append('eaten')\n",
    "stop_words.append('matrix')\n",
    "stop_words.append('extraterrestrial')\n",
    "stop_words.append('wi')\n",
    "stop_words.append('ivt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:33.133879Z",
     "start_time": "2019-06-14T04:24:33.127745Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_to(stop_words,'stop_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:41.469651Z",
     "start_time": "2019-06-14T04:24:33.136889Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data['tokenized'] = text_data['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# Remove words that have less than 3 characters\n",
    "text_data['text'] = text_data['text'].str.replace(r'\\b(\\w{1,2})\\b', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:41.610184Z",
     "start_time": "2019-06-14T04:24:41.471336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column that has the length of each text\n",
    "\n",
    "text_data['token_length'] = text_data.apply(lambda row: len(row['tokenized']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:41.667622Z",
     "start_time": "2019-06-14T04:24:41.612679Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data[text_data['text'].str.contains('phrase block')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:41.728905Z",
     "start_time": "2019-06-14T04:24:41.670646Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data[text_data.text.str.contains('automatically')].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:41.750118Z",
     "start_time": "2019-06-14T04:24:41.731136Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T04:24:42.839570Z",
     "start_time": "2019-06-14T04:24:41.751907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pickling\n",
    "\n",
    "pickle_to(text_data,'text_data.pkl')\n",
    "pickle_to(data,'data.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:57:10.040372Z",
     "start_time": "2019-06-20T17:57:09.854631Z"
    }
   },
   "outputs": [],
   "source": [
    "raw[raw['label'] == 'REAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T17:41:16.584597Z",
     "start_time": "2019-06-19T17:41:16.428462Z"
    }
   },
   "outputs": [],
   "source": [
    "raw = pickle_from('raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:56:58.389192Z",
     "start_time": "2019-06-20T17:56:58.111730Z"
    }
   },
   "outputs": [],
   "source": [
    "raw[raw['label'] == 'FAKE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
